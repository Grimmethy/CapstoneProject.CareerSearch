{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "#### I need to build a function that scrapes glassdoor.com for relevant job information.\n",
    "#### I willl start by pulling together specific job listings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I will be borrowing heavily from picklesueat's scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'Data Analyst'\n",
    "url = \"https://www.glassdoor.com/blog/tag/job-search/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs keyword and loc, and bring you to the Job Listings\n",
    "def go_to_listings(driver, loc):\n",
    "\n",
    "    \n",
    "       # wait for the search bar to appear\n",
    "    element = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//*[@id='sc.keyword']\"))\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        # look for search bar field\n",
    "        position_field = driver.find_element_by_xpath(\"//*[@id='sc.keyword']\")\n",
    "        location_field = driver.find_element_by_xpath(\"//*[@id='sc.location']\")\n",
    "        \n",
    "\n",
    "        # fill in with pre-defined data\n",
    "        position_field.clear()\n",
    "        position_field.send_keys(keyword)\n",
    "        \n",
    "        location_field.clear()\n",
    "        location_field.send_keys(loc)\n",
    "\n",
    "        # wait for a little so location gets set\n",
    "        time.sleep(3)\n",
    "        driver.find_element_by_xpath(\" //*[@id='HeroSearchButton']\").click()\n",
    "\n",
    "        # close a random popup if it shows up\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"//*[@id='JAModal']/div/div[2]/span\").click()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(3)\n",
    "        \n",
    "            \n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"//*[@id='JAModal']/div/div[2]/span\").click()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "  \n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "#Initializes driver, and navigates to URL\n",
    "def initialize_driver(path):\n",
    "    #Initializing the webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "\n",
    "    ua = UserAgent()\n",
    "    userAgent = ua.random\n",
    "    options.add_argument(f'user-agent={userAgent}')\n",
    "    \n",
    "    #makes the driver more 'undetectale'\n",
    "    options.add_argument(\"--disable-blink-features\"), options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "    #Uncomment the line below if you'd like to scrape without a new Chrome window every time.\n",
    "    #options.add_argument('headless')\n",
    "    \n",
    "    #Change the path to where chromedriver is in your home folder.\n",
    "    driver = webdriver.Chrome(executable_path=path, options=options)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    \n",
    "    return driver\n",
    "\n",
    "def Xout_pop_ups(driver):\n",
    "        #Test for the \"Sign Up\" prompt and get rid of it.\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        driver.find_element_by_class_name(\"selected\").click()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    time.sleep(.1)\n",
    "\n",
    "    #X out pop-up\n",
    "    try:\n",
    "        driver.find_element_by_css_selector('[alt=\"Close\"]').click() #clicking to the X.\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def click_listing(driver, job_button):\n",
    "    illstatus = 'Fine'\n",
    "    try:\n",
    "        job_button.click()#You might\n",
    "        \n",
    "    except Exception:\n",
    "        illstatus = 'Ill'\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            job_button.click()\n",
    "            illstatus = 'Illness Corrected'\n",
    "        except Exception:\n",
    "            illstatus = 'Failed'\n",
    "        \n",
    "    return illstatus\n",
    "        \n",
    "    \n",
    "def collect_listing_info(driver, slp_time, verbose):\n",
    "    time.sleep(slp_time)\n",
    "    collected_successfully = False\n",
    "    no_load_time = 0\n",
    "    while not collected_successfully:\n",
    "        #If page has timed out from not-loading, restart the process by clicking first listing\n",
    "\n",
    "        if(no_load_time >= 10):\n",
    "            driver.find_element_by_xpath(\"//*[@id='MainCol']/div[1]/ul/li[1]\").click()\n",
    "\n",
    "            \n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"//*[@id='HeroHeaderModule']/div[3]/div[2]/div/div[1]/div[2]/button\")\n",
    "            \n",
    "        except NoSuchElementException:\n",
    "            no_load_time +=1\n",
    "        try:\n",
    "            company_name = driver.find_element_by_xpath('.//div[@class=\"employerName\"]').text\n",
    "            location = driver.find_element_by_xpath('.//div[@class=\"location\"]').text\n",
    "            job_title = driver.find_element_by_xpath('.//div[contains(@class, \"title\")]').text\n",
    "            job_description = driver.find_element_by_xpath('.//div[@class=\"jobDescriptionContent desc\"]').text\n",
    "            collected_successfully = True\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "\n",
    "    try:\n",
    "        salary_estimate = driver.find_element_by_xpath('.//span[@class=\"gray salary\"]').text\n",
    "    except NoSuchElementException:\n",
    "        salary_estimate = -1 #You need to set a \"not found value. It's important.\"\n",
    "    \n",
    "    try:\n",
    "        rating = driver.find_element_by_xpath('.//span[@class=\"rating\"]').text\n",
    "    except NoSuchElementException:\n",
    "        rating = -1 #You need to set a \"not found value. It's important.\"\n",
    "\n",
    "    #Printing for debugging\n",
    "    if verbose:\n",
    "        print(\"Job Title: {}\".format(job_title))\n",
    "        print(\"Salary Estimate: {}\".format(salary_estimate))\n",
    "        print(\"Job Description: {}\".format(job_description[:500]))\n",
    "        print(\"Rating: {}\".format(rating))\n",
    "        print(\"Company Name: {}\".format(company_name))\n",
    "        print(\"Location: {}\".format(location))\n",
    "\n",
    "    #Going to the Company tab...\n",
    "    #clicking on this:\n",
    "    #<div class=\"tab\" data-tab-type=\"overview\"><span>Company</span></div>\n",
    "    try:\n",
    "        driver.find_element_by_xpath('.//div[@class=\"tab\" and @data-tab-type=\"overview\"]').click()\n",
    "\n",
    "        try:\n",
    "            #<div class=\"infoEntity\">\n",
    "            #    <label>Headquarters</label>\n",
    "            #    <span class=\"value\">San Francisco, CA</span>\n",
    "            #</div>\n",
    "            headquarters = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Headquarters\"]//following-sibling::*').text\n",
    "        except NoSuchElementException:\n",
    "            headquarters = -1\n",
    "\n",
    "        try:\n",
    "            size = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Size\"]//following-sibling::*').text\n",
    "        except NoSuchElementException:\n",
    "            size = -1\n",
    "\n",
    "        try:\n",
    "            founded = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Founded\"]//following-sibling::*').text\n",
    "        except NoSuchElementException:\n",
    "            founded = -1\n",
    "\n",
    "        try:\n",
    "            type_of_ownership = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Type\"]//following-sibling::*').text\n",
    "        except NoSuchElementException:\n",
    "            type_of_ownership = -1\n",
    "\n",
    "        try:\n",
    "            industry = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Industry\"]//following-sibling::*').text\n",
    "        except NoSuchElementException:\n",
    "            industry = -1\n",
    "\n",
    "        try:\n",
    "            sector = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Sector\"]//following-sibling::*').text\n",
    "        except NoSuchElementException:\n",
    "            sector = -1\n",
    "\n",
    "        try:\n",
    "            revenue = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Revenue\"]//following-sibling::*').text\n",
    "        except NoSuchElementException:\n",
    "            revenue = -1\n",
    "\n",
    "        try:\n",
    "            competitors = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Competitors\"]//following-sibling::*').text\n",
    "        except NoSuchElementException:\n",
    "            competitors = -1\n",
    "\n",
    "        \n",
    "        try:\n",
    "            easy_apply = (driver.find_element_by_xpath('//*[@id=\"HeroHeaderModule\"]/div[3]/div[2]/div/div[1]/div[1]/button/span').text  == 'Easy Apply')\n",
    "        except NoSuchElementException:\n",
    "            easy_apply = -1\n",
    "\n",
    "    except NoSuchElementException:  #Rarely, some job postings do not have the \"Company\" tab.\n",
    "        headquarters = -1\n",
    "        size = -1\n",
    "        founded = -1\n",
    "        type_of_ownership = -1\n",
    "        industry = -1\n",
    "        sector = -1\n",
    "        revenue = -1\n",
    "        competitors = -1\n",
    "        easy_apply = -1\n",
    "\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Headquarters: {}\".format(headquarters))\n",
    "        print(\"Size: {}\".format(size))\n",
    "        print(\"Founded: {}\".format(founded))\n",
    "        print(\"Type of Ownership: {}\".format(type_of_ownership))\n",
    "        print(\"Industry: {}\".format(industry))\n",
    "        print(\"Sector: {}\".format(sector))\n",
    "        print(\"Revenue: {}\".format(revenue))\n",
    "        print(\"Competitors: {}\".format(competitors))\n",
    "        print(\"Easy Apply: {}\".format(easy_apply))\n",
    "        print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "\n",
    "    jobinfo = ({\"Job Title\" : job_title,\n",
    "    \"Salary Estimate\" : salary_estimate,\n",
    "    \"Job Description\" : job_description,\n",
    "    \"Rating\" : rating,\n",
    "    \"Company Name\" : company_name,\n",
    "    \"Location\" : location,\n",
    "    \"Headquarters\" : headquarters,\n",
    "    \"Size\" : size,\n",
    "    \"Founded\" : founded,\n",
    "    \"Type of ownership\" : type_of_ownership,\n",
    "    \"Industry\" : industry,\n",
    "    \"Sector\" : sector,\n",
    "    \"Revenue\" : revenue,\n",
    "    \"Competitors\" : competitors,\n",
    "    \"Easy Apply\": easy_apply})\n",
    "    \n",
    "    return jobinfo\n",
    "\n",
    "def make_sure_jobs(driver):\n",
    "    try:\n",
    "        if(driver.find_element_by_xpath(\"//*[@id='MainCol']/div[1]/div[2]/div/div[1]/p/span[2]\").text.split(' ' )[1] == 'search'):\n",
    "            driver.refresh()\n",
    "            time.sleep(5)\n",
    "            if(driver.find_element_by_xpath(\"//*[@id='MainCol']/div[1]/div[2]/div/div[1]/p/span[2]\").text.split(' ' )[1] == 'search'):\n",
    "                return 'X'\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        print(driver.find_element_by_xpath(\"//*[@id='MainCol']/div[1]/div[2]/div/div/h4\").text.split(' ' )[1])\n",
    "        if((driver.find_element_by_xpath(\"//*[@id='MainCol']/div[1]/div[2]/div/div/h4\").text.split(' ' )[1] == 'search')):\n",
    "            driver.refresh()\n",
    "            time.sleep(5)\n",
    "            if((driver.find_element_by_xpath(\"//*[@id='MainCol']/div[1]/div[2]/div/div/h4\").text.split(' ' )[1] == 'search')):\n",
    "                return 'X'\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "def get_jobs(keyword, num_jobs, verbose, path, slp_time, loc = ' ', driver = ' '):\n",
    "    \n",
    "    '''Gathers jobs as a dataframe, scraped from Glassdoor'''\n",
    "    \n",
    "    count = 0 \n",
    "\n",
    "    if(loc == ' '):\n",
    "        driver = initialize_driver(path)\n",
    "    \n",
    "    driver.get(url)\n",
    "    go_to_listings(driver,loc)\n",
    "           \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    jobs = []\n",
    "\n",
    "    while len(jobs) < num_jobs:  #If true, should be still looking for new jobs.\n",
    "\n",
    "        #Let the page load. Change this number based on your internet speed.\n",
    "        #Or, wait until the webpage is loaded, instead of hardcoding it.\n",
    "\n",
    "        element = WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//*[@id='HeroSearchButton']\"))\n",
    "        )\n",
    "\n",
    "\n",
    "        #Test for the \"Sign Up\" prompt and get rid of it.\n",
    "        Xout_pop_ups(driver)\n",
    "\n",
    "        x = make_sure_jobs(driver)\n",
    "        if(x == 'X'):\n",
    "            break\n",
    "\n",
    "        #Going through each job in this page\n",
    "        time.sleep(1.5)\n",
    "        job_buttons = driver.find_elements_by_css_selector(\"li.jl.react-job-listing.gdGrid\")  #jl for Job Listing. These are the buttons we're going to click.\n",
    "        if(len(job_buttons) <=25):\n",
    "            print(\"JOBS PER PAGE: \" + str(len(job_buttons)))\n",
    "        for job_button in job_buttons: \n",
    "            if len(jobs) >= num_jobs:\n",
    "                print(\"Job Target Reached:\" + str(len(jobs)) + '/' + str(num_jobs))\n",
    "                break\n",
    "\n",
    "            status = click_listing(driver, job_button)\n",
    "\n",
    "            #Weird error pops up here, so I'm on the lookout,\n",
    "            if(status == 'Failed'):\n",
    "                print(status)\n",
    "                break\n",
    "\n",
    "            #append listing data, to total list for that location \n",
    "            jobs.append(collect_listing_info(driver, slp_time, verbose = verbose))\n",
    "\n",
    "        page_num = driver.find_element_by_xpath(\"//*[@id='ResultsFooter']/div[1]\").text\n",
    "        page_num = page_num.split(' ')\n",
    "        if(page_num[1] == page_num[3]):\n",
    "            break\n",
    "        \n",
    "        #Clicking on the \"next page\" button\n",
    "        try:\n",
    "            if(count<=33):\n",
    "                driver.find_element_by_xpath('.//li[@class=\"next\"]//a').click()\n",
    "                count +=1\n",
    "        except NoSuchElementException:\n",
    "            print(\"Scraping terminated before reaching target number of jobs. Needed {}, got {}.\".format(num_jobs, len(jobs)))\n",
    "            break\n",
    "\n",
    "    \n",
    "    return pd.DataFrame(jobs)  #This line converts the dictionary object into a pandas DataFrame.\n",
    "\n",
    "\n",
    "def page_limit(keyword, num_jobs, verbose, path, slp_time, start_at = 0):\n",
    "    driver = initialize_driver(path)\n",
    "    #General Location\n",
    "    if(start_at == 0):\n",
    "        df = get_jobs(keyword, 1, verbose, path, slp_time)\n",
    "        df = df[0:0]\n",
    "        df.to_csv('Jobs_apply/glassdoor_job_data/' + keyword.replace(' ', '') + '.csv', index = False)\n",
    "    \n",
    "    cities = top_cities()\n",
    "    \n",
    "    for x in range(start_at,len(cities.index) - 217):\n",
    "        print(\"City Index: \" + str(x))\n",
    "        try:    \n",
    "            df = get_jobs(keyword, num_jobs, verbose, path, slp_time, loc = (cities.iloc[x,0] + ', ' + cities.iloc[x,1]), driver = driver)\n",
    "        except Exception:\n",
    "            print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            df = get_jobs(keyword, num_jobs, verbose, path, slp_time, loc = (cities.iloc[x,0] + ', ' + cities.iloc[x,1]), driver = driver)\n",
    "        df.to_csv('Jobs_apply/glassdoor_job_data/' + keyword.replace(' ', '') + '.csv', index = False, mode='a', header=False)\n",
    "\n",
    "\n",
    "\n",
    "def top_cities():\n",
    "    #scrapes the list of most populated cities from wikipedia\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\"\n",
    "    page = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    table = soup.find('table',{'class':'wikitable sortable'})\n",
    "    rows = (table.find_all('tr'))\n",
    "    \n",
    "    city = []\n",
    "    state = []\n",
    "    df = pd.DataFrame()\n",
    "    rows.pop(0)\n",
    "    for row in rows: \n",
    "        data = row.find_all('a')\n",
    "        city.append(data[0].text)\n",
    "        state.append(row.find_all('td')[2].text.replace('\\n',''))\n",
    "        \n",
    "    city = pd.Series(city)\n",
    "    state = pd.Series(state)\n",
    "    df['City'] = city\n",
    "    df['State'] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(glassdoorUrl):\n",
    "    response = requests.get(glassdoorUrl)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    profile = soup.findAll('ul', class_=\"jlGrid hover p-0 \")\n",
    "    rows = []\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are the variable used for identifying what specific url to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a temporary setting. It uses \"Data Science\" as a search query.\n",
    "glassdoorUrl = 'https://www.glassdoor.com/Job/jobs.htm?sc.keyword=data%20science&locT=C&locId=1150481'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n"
     ]
    }
   ],
   "source": [
    "scrape(glassdoorUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glassdoorscraper as gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'glassdoorscraper' from 'C:\\\\Users\\\\Grimmethy\\\\Flatironcoursework\\\\capstone\\\\CapstoneProject.CareerSearch\\\\glassdoorscraper.py'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
